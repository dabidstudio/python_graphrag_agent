#!/usr/bin/env python3
"""
Standalone Knowledge Graph Generator
====================================

This script combines data collection and processing into a single executable file.
It collects episode data from Wikipedia and processes it into a knowledge graph.

Usage:
    uv run python standalone_knowledge_graph.py
    
    or simply:
    
    uv run standalone_knowledge_graph.py

Requirements:
    - OpenAI API key in .env file or OPENAI_API_KEY environment variable
    - Internet connection for Wikipedia scraping and OpenAI API calls
"""

import json
import re
import os
from typing import List, Dict, Any, Optional, Union
import requests
from bs4 import BeautifulSoup
import openai
from dotenv import load_dotenv
from pydantic import BaseModel

# Load environment variables
load_dotenv()

# Initialize OpenAI client
client = openai.OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# Type definitions
PropertyValue = Union[str, int, float, bool, None]

class Node(BaseModel):
    id: str
    label: str
    properties: Optional[Dict[str, PropertyValue]] = None

class Relationship(BaseModel):
    type: str
    start_node_id: str
    end_node_id: str
    properties: Optional[Dict[str, PropertyValue]] = None

class GraphResponse(BaseModel):
    nodes: List[Node]
    relationships: List[Relationship]

# Templates for LLM processing
UPDATED_TEMPLATE = """
You are a top-tier algorithm designed for extracting information in structured formats to build a knowledge graph. Extract the entities (nodes) and specify their type from the following text, but **you MUST select nodes ONLY from the following predefined set** (see the provided NODES list below). Do not create any new nodes or use names that do not exactly match one in the NODES list.

Also extract the relationships between these nodes. Return the result as JSON using the following format:

{
  "nodes": [
    {"id": "N0", "label": "ì¸ê°„", "properties": {"name": "Tanjiro Kamado"}}
  ],
  "relationships": [
    {"type": "FIGHTS", "start_node_id": "N0", "end_node_id": "N13", "properties": {"outcome": "victory"}}
  ]
}

Additional rules:
- Use only nodes from the NODES list. Do not invent or substitute nodes.
- Skip any relationship if one of its entities is not in NODES.
- Only output valid relationships where both endpoints exist in NODES and the direction matches their types.

NODES =
[
  {"id":"N0",  "label":"ì¸ê°„", "properties":{"name":"Tanjiro Kamado"}},
  {"id":"N1",  "label":"ì¸ê°„", "properties":{"name":"Nezuko Kamado"}},
  {"id":"N2",  "label":"ì¸ê°„", "properties":{"name":"Giyu Tomioka"}},
  {"id":"N3",  "label":"ì¸ê°„", "properties":{"name":"Sakonji Urokodaki"}},
  {"id":"N4",  "label":"ì¸ê°„", "properties":{"name":"Sabito"}},
  {"id":"N5",  "label":"ì¸ê°„", "properties":{"name":"Makomo"}},
  {"id":"N6",  "label":"ì¸ê°„", "properties":{"name":"Zenitsu Agatsuma"}},
  {"id":"N7",  "label":"ì¸ê°„", "properties":{"name":"Inosuke Hashibira"}},
  {"id":"N8",  "label":"ì¸ê°„", "properties":{"name":"Kanao Tsuyuri"}},
  {"id":"N9",  "label":"ì¸ê°„", "properties":{"name":"Kyojuro Rengoku"}},
  {"id":"N10", "label":"ì¸ê°„", "properties":{"name":"Kagaya Ubuyashiki"}},
  {"id":"N11", "label":"ì¸ê°„", "properties":{"name":"Shinobu Kocho"}},
  {"id":"N12", "label":"ì¸ê°„", "properties":{"name":"Sanemi Shinazugawa"}},
  {"id":"N13", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Muzan Kibutsuji"}},
  {"id":"N14", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Susamaru"}},
  {"id":"N15", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Yahaba"}},
  {"id":"N16", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Kyogai"}},
  {"id":"N17", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Rui"}},
  {"id":"N18", "label":"ë„ê¹¨ë¹„", "properties":{"name":"Enmu"}}
]
"""

# Korean node name mapping
KOREAN_NODE_MAP = {
    # ê·€ì‚´ëŒ€ (Demon Slayer Corps)
    "Tanjiro Kamado": "ì¹´ë§ˆë„ íƒ„ì§€ë¡œ",
    "Nezuko Kamado": "ì¹´ë§ˆë„ ë„¤ì¦ˆì½”",
    "Giyu Tomioka": "í† ë¯¸ì˜¤ì¹´ ê¸°ìœ ",
    "Sakonji Urokodaki": "ìš°ë¡œì½”ë‹¤í‚¤ ì‚¬ì½˜ì§€",
    "Sabito": "ì‚¬ë¹„í† ",
    "Makomo": "ë§ˆì½”ëª¨",
    "Zenitsu Agatsuma": "ì•„ê°€ì¸ ë§ˆ ì  ì´ì¸ ",
    "Inosuke Hashibira": "í•˜ì‹œë¹„ë¼ ì´ë…¸ìŠ¤ì¼€",
    "Kanao Tsuyuri": "ì¸ ìœ ë¦¬ ì¹´ë‚˜ì˜¤",
    "Kyojuro Rengoku": "ë Œê³ ì¿  ì¿„ì¥¬ë¡œ",
    "Kagaya Ubuyashiki": "ìš°ë¶€ì•¼ì‹œí‚¤ ì¹´ê°€ì•¼",
    "Shinobu Kocho": "ì½”ìµ¸ìš° ì‹œë…¸ë¶€",
    "Sanemi Shinazugawa": "ì‹œë‚˜ì¦ˆê°€ì™€ ì‚¬ë„¤ë¯¸",

    # ë„ê¹¨ë¹„ (Demons)
    "Muzan Kibutsuji": "í‚¤ë¶€ì¸ ì§€ ë¬´ì”",
    "Susamaru": "ìŠ¤ì‚¬ë§ˆë£¨",
    "Yahaba": "ì•¼í•˜ë°”",
    "Kyogai": "ì¿„ìš°ê°€ì´",
    "Rui": "ë£¨ì´",
    "Enmu": "ì—”ë¬´",
}

def llm_call_structured(prompt: str, model: str = "gpt-4o-mini") -> GraphResponse:
    """Call OpenAI API with structured output"""
    resp = client.beta.chat.completions.parse(
        model=model,
        messages=[
            {"role": "user", "content": prompt},
        ],
        response_format=GraphResponse,
    )
    return resp.choices[0].message.parsed

def combine_chunk_graphs(chunk_graphs: List[GraphResponse]) -> GraphResponse:
    """
    Combine multiple GraphResponse objects into one.
    - Collects all nodes and relationships
    - Removes duplicate nodes, keeping the first occurrence
    """
    # 1. Collect all nodes from all chunk graphs
    all_nodes = []
    for chunk_graph in chunk_graphs:
        for node in chunk_graph.nodes:
            all_nodes.append(node)
    
    # 2. Collect all relationships from all chunk graphs
    all_relationships = []
    for chunk_graph in chunk_graphs:
        for relationship in chunk_graph.relationships:
            all_relationships.append(relationship)
    
    # 3. Remove duplicate nodes
    unique_nodes = []
    seen = set()  # Set to remember already added nodes

    for node in all_nodes:
        # Create a key from node's id, label, and properties
        node_key = (node.id, node.label, str(node.properties))
        # Add to unique_nodes if not already seen
        if node_key not in seen:
            unique_nodes.append(node)
            seen.add(node_key)

    # 4. Return combined GraphResponse
    return GraphResponse(nodes=unique_nodes, relationships=all_relationships)

def fetch_episode(link: str) -> List[dict]:
    """Fetch episode data from Wikipedia"""
    season = int(re.search(r"season_(\d+)", link).group(1))
    print(f"Fetching Season {season} from: {link}")
    headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)"}
    response = requests.get(link, headers=headers)
    
    soup = BeautifulSoup(response.text, "html.parser")
    table = soup.select_one("table.wikitable.plainrowheaders.wikiepisodetable")

    episodes = []
    rows = table.select("tr.vevent.module-episode-list-row")

    for i, row in enumerate(rows, start=1):
        synopsis = None
        synopsis_row = row.find_next_sibling("tr", class_="expand-child")
        if synopsis_row:
            synopsis_cell = synopsis_row.select_one("td.description div.shortSummaryText")
            synopsis = synopsis_cell.get_text(strip=True) if synopsis_cell else None

        episodes.append({
            "season": season,
            "episode_in_season": i,
            "synopsis": synopsis,
        })
    
    return episodes

def collect_data() -> List[dict]:
    """Collect episode data from multiple seasons"""
    print("=== ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘ ===")
    
    episode_links = [
        "https://en.wikipedia.org/wiki/Demon_Slayer:_Kimetsu_no_Yaiba_season_1",  # ê·€ë©¸ì˜ ì¹¼ë‚  ì‹œì¦Œ 1
        # Add more seasons as needed:
        # "https://en.wikipedia.org/wiki/Demon_Slayer:_Kimetsu_no_Yaiba_season_2",  # ê·€ë©¸ì˜ ì¹¼ë‚  ì‹œì¦Œ 2
    ]
    
    all_episodes = []
    for link in episode_links:
        try:
            episodes = fetch_episode(link)
            all_episodes.extend(episodes)
        except Exception as e:
            print(f"Error fetching data from {link}: {e}")
            continue
    
    print(f"ì´ {len(all_episodes)}ê°œ ì—í”¼ì†Œë“œ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_episodes

def process_data(episodes: List[dict]) -> GraphResponse:
    """Process episode data into knowledge graph"""
    print("=== ë°ì´í„° ì²˜ë¦¬ ì‹œì‘ ===")
    
    chunk_graphs: List[GraphResponse] = []
    
    for episode in episodes:
        if not episode.get("synopsis"):
            print(f"ì—í”¼ì†Œë“œ S{episode['season']}E{episode['episode_in_season']:02d}: ì‹œë†‰ì‹œìŠ¤ê°€ ì—†ì–´ ê±´ë„ˆëœ€")
            continue
            
        print(f"ì—í”¼ì†Œë“œ ì²˜ë¦¬ ì¤‘: ì‹œì¦Œ {episode['season']}, ì—í”¼ì†Œë“œ {episode['episode_in_season']}")
        
        try:
            # (1) Generate prompt with updated template for node standardization
            prompt = UPDATED_TEMPLATE + f"\n ì…ë ¥ê°’\n {episode['synopsis']}"
            graph_response = llm_call_structured(prompt)

            # (2) Add episode number to relationships (e.g., S1E01)
            episode_number = f"S{episode['season']}E{episode['episode_in_season']:02d}"

            for relationship in graph_response.relationships:
                if relationship.properties is None:
                    relationship.properties = {}
                relationship.properties["episode_number"] = episode_number
                
            # (3) Convert node names to Korean
            for node in graph_response.nodes:
                english_name = node.properties.get("name", "")
                if english_name in KOREAN_NODE_MAP:
                    node.properties["name"] = KOREAN_NODE_MAP[english_name]
            
            chunk_graphs.append(graph_response)
            
        except Exception as e:
            print(f"  - ì—í”¼ì†Œë“œ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            continue
    
    if not chunk_graphs:
        raise Exception("ê·¸ë˜í”„ë¥¼ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œí•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.")
    
    print(f"ì´ {len(chunk_graphs)}ê°œ ì—í”¼ì†Œë“œ ì²˜ë¦¬ ì™„ë£Œ")
    return combine_chunk_graphs(chunk_graphs)

def save_output(episodes: List[dict], final_graph: GraphResponse):
    """Save outputs to JSON files"""
    print("=== ê²°ê³¼ ì €ì¥ ===")
    
    # Create output directory if it doesn't exist
    os.makedirs("output", exist_ok=True)
    
    # Save original data
    with open("output/1_ì›ë³¸ë°ì´í„°.json", "w", encoding="utf-8") as f:
        json.dump(episodes, f, indent=2, ensure_ascii=False)
    print("ì›ë³¸ ë°ì´í„° ì €ì¥: output/1_ì›ë³¸ë°ì´í„°.json")
    
    # Save final knowledge graph
    with open("output/ì§€ì‹ê·¸ë˜í”„_ìµœì¢….json", "w", encoding="utf-8") as f:
        json.dump(final_graph.model_dump(), f, ensure_ascii=False, indent=2)
    print("ìµœì¢… ì§€ì‹ê·¸ë˜í”„ ì €ì¥: output/ì§€ì‹ê·¸ë˜í”„_ìµœì¢….json")

def main():
    """Main function that orchestrates the entire process"""
    try:
        # Check for OpenAI API key
        if not os.getenv("OPENAI_API_KEY"):
            print("âŒ OPENAI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("\nì„¤ì • ë°©ë²•:")
            print("1. .env íŒŒì¼ì— OPENAI_API_KEY=your_api_key_here ì¶”ê°€")
            print("2. ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •: export OPENAI_API_KEY=your_api_key_here")
            return 1
        
        print("ğŸš€ ì§€ì‹ê·¸ë˜í”„ ìƒì„±ê¸° ì‹œì‘")
        print("=" * 50)
        
        # Step 1: Collect data
        episodes = collect_data()
        
        if not episodes:
            raise Exception("ìˆ˜ì§‘ëœ ì—í”¼ì†Œë“œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        
        # Step 2: Process data
        final_graph = process_data(episodes)
        
        # Step 3: Save outputs
        save_output(episodes, final_graph)
        
        print("=" * 50)
        print("âœ… ì§€ì‹ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ!")
        print(f"ğŸ“Š ì´ ë…¸ë“œ ìˆ˜: {len(final_graph.nodes)}")
        print(f"ğŸ”— ì´ ê´€ê³„ ìˆ˜: {len(final_graph.relationships)}")
        print("\nìƒì„±ëœ íŒŒì¼:")
        print("- output/1_ì›ë³¸ë°ì´í„°.json")
        print("- output/ì§€ì‹ê·¸ë˜í”„_ìµœì¢….json")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return 1
    
    return 0

if __name__ == "__main__":
    exit(main())
